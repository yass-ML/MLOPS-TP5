{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ea92dfd",
   "metadata": {},
   "source": [
    "# C-Transpiled model VS Joblib model : Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211c0fcb",
   "metadata": {},
   "source": [
    "Quick notebook to verify that the implemented transpiler works as intended. We will work with:\n",
    "* `LinearRegression`\n",
    "* `LogisticRegression` (binary)\n",
    "* `DecisionTreeClassifier` (binary)\n",
    "* `DecisionTreeRegressor`\n",
    "  \n",
    "On the following datasets:\n",
    "* `Houses` dataset (regression)\n",
    "* `Breast Cancer` dataset (classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86f3cf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import subprocess\n",
    "import json\n",
    "from transpile_simple_model import LinearModelTranspiler\n",
    "from sklearn.metrics import r2_score, accuracy_score\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "HOUSE_DATA_PATH = \"data/houses.csv\"\n",
    "CANCER_DATA_PATH = \"data/breast-cancer.csv\" # https://www.kaggle.com/datasets/yasserh/breast-cancer-dataset?resource=download\n",
    "\n",
    "LIN_REG_PATH = \"data/regression.joblib\"\n",
    "LOG_REG_PATH = \"data/logistic_regression.joblib\"\n",
    "\n",
    "DECISION_TREE_CLF_PATH = \"data/tree_clf.joblib\"\n",
    "DECISION_TREE_REG_PATH = \"data/tree_reg.joblib\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0a118e",
   "metadata": {},
   "source": [
    "## Util Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f140858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_c_dataset_string(X_df, n_samples=None):\n",
    "    \"\"\"\n",
    "    Convert pandas DataFrame to C 2D array string format.\n",
    "    \n",
    "    Args:\n",
    "        X_df: pandas DataFrame with features\n",
    "        n_samples: number of samples to include (None = all)\n",
    "    \n",
    "    Returns:\n",
    "        C code string for 2D array declaration\n",
    "    \"\"\"\n",
    "    if n_samples is None:\n",
    "        n_samples = len(X_df)\n",
    "    \n",
    "    X_subset = X_df.iloc[:n_samples]\n",
    "    n_features = X_df.shape[1]\n",
    "    \n",
    "    # Start building the C array\n",
    "    c_array = f\"float dataset[{n_samples}][{n_features}] = {{\\n\"\n",
    "    \n",
    "    for i, row in X_subset.iterrows():\n",
    "        values = \", \".join([f\"{val}f\" for val in row.values])\n",
    "        c_array += f\"    {{{values}}}\"\n",
    "        if i < n_samples - 1:\n",
    "            c_array += \",\"\n",
    "        c_array += \"\\n\"\n",
    "    \n",
    "    c_array += \"};\"\n",
    "    return c_array\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c34d72db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_c_main_function(X_df, y_series, n_samples=None):\n",
    "    \"\"\"\n",
    "    Build a complete C main function computes predictions with the transpiled model.\n",
    "    \n",
    "    Args:\n",
    "        X_df: pandas DataFrame with features\n",
    "        y_series: pandas Series with true target values\n",
    "        n_samples: number of samples to test (None = all)\n",
    "    \n",
    "    Returns:\n",
    "        Complete C main function as a string\n",
    "    \"\"\"\n",
    "    if n_samples is None:\n",
    "        n_samples = len(X_df)\n",
    "    \n",
    "    n_features = X_df.shape[1]\n",
    "    \n",
    "    # Build the main function\n",
    "    main_func = f\"\"\"\n",
    "int main() {{\n",
    "    // Dataset with {n_samples} samples and {n_features} features\n",
    "    {build_c_dataset_string(X_df, n_samples)}\n",
    "    \n",
    "    // True for comparison\n",
    "    float y_true[] = {{{\", \".join([f\"{y_series.iloc[i]}f\" for i in range(n_samples)])}}};\n",
    "    \n",
    "    // Test predictions for each sample\n",
    "    printf(\"[%f\", prediction(dataset[0]));\n",
    "    for (int i = 1; i < {n_samples}; i++) {{\n",
    "        float y_pred = prediction(dataset[i]);\n",
    "        printf(\", %f\", y_pred);\n",
    "    }}\n",
    "    printf(\"]\");\n",
    "    \n",
    "    return 0;\n",
    "}}\n",
    "\"\"\"\n",
    "    return main_func\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b977f125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_complete_c_file(original_c_file, output_c_file, X_df, y_series, n_samples=5):\n",
    "    \"\"\"\n",
    "    Create a complete C file with the original model code + main function.\n",
    "    \n",
    "    Args:\n",
    "        original_c_file: path to the original transpiled C file\n",
    "        output_c_file: path for the new complete C file\n",
    "        X_df: pandas DataFrame with features\n",
    "        y_series: pandas Series with true target values\n",
    "        n_samples: number of samples to test\n",
    "    \"\"\"\n",
    "    # Read the original C file\n",
    "    with open(original_c_file, 'r') as f:\n",
    "        original_content = f.read()\n",
    "    \n",
    "    # Generate the main function\n",
    "    main_function = build_c_main_function(X_df, y_series, n_samples)\n",
    "    \n",
    "    # Combine them\n",
    "    complete_content = original_content + \"\\n\" + main_function\n",
    "    \n",
    "    # Write to new file\n",
    "    with open(output_c_file, 'w') as f:\n",
    "        f.write(complete_content)\n",
    "    \n",
    "    print(f\"* Created complete C file: {output_c_file}\")\n",
    "    print(f\"  - Original code from: {original_c_file}\")\n",
    "    print(f\"  - Added main function with {n_samples} test samples\")\n",
    "    \n",
    "    return complete_content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95531c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_eval(model: BaseEstimator | str, X: pd.DataFrame, y: pd.Series, metric = None, verbose=True):\n",
    "    \"\"\"\n",
    "    Predict on a dataset using either a C transpiled model or a Python model, optionally evaluating a metric\n",
    "    Args:\n",
    "        model: either the sklearn BaseEstimator object or the path to the C executable\n",
    "        X: dataset to predict on\n",
    "        y: labels or targets for dataset X\n",
    "        metric: sklearn's metric function\n",
    "    \"\"\"\n",
    "    if isinstance(model, BaseEstimator): # Use python\n",
    "        y_pred = model.predict(X)\n",
    "        if metric is not None:\n",
    "            m = metric(y_true=y.to_numpy(), y_pred = y_pred)\n",
    "            print(f\"model's {metric.__name__} (Python): {m}\")\n",
    "\n",
    "    elif isinstance(model, str): # Use transpiled C\n",
    "        result = subprocess.run(['./test_model'], capture_output=True, text=True)\n",
    "        output = result.stdout\n",
    "        y_pred = json.loads(output)\n",
    "        if metric is not None:\n",
    "            m = metric(y_true=y.to_numpy(), y_pred = y_pred)\n",
    "            print(f\"model's {metric.__name__} (C): {m}\")\n",
    "    else:\n",
    "        raise ValueError(\"model must be either a BaseEstimator or a string path to the C executable\")\n",
    "    \n",
    "\n",
    "    if verbose:\n",
    "        for i in range(5):\n",
    "            print(f\"Sample {i+1}: True = {y.iloc[i]:.2f}, Predicted = {y_pred[i]:.2f}\")\n",
    "\n",
    "    return y_pred\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48a65c8",
   "metadata": {},
   "source": [
    "## Linear Regression Comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2544dce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(HOUSE_DATA_PATH)\n",
    "X = data.drop(columns=[\"price\", \"orientation\"])\n",
    "y = data[\"price\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e83a260",
   "metadata": {},
   "source": [
    "### C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bf50fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from data/regression.joblib...\n",
      "\n",
      "C code generated and saved to: linear_model.c\n",
      "* Created complete C file: linear_model_with_main.c\n",
      "  - Original code from: linear_model.c\n",
      "  - Added main function with 40 test samples\n"
     ]
    }
   ],
   "source": [
    "transpiler = LinearModelTranspiler(LIN_REG_PATH, output_c_file=\"linear_model.c\")\n",
    "transpiler.transpile()\n",
    "\n",
    "# Create the complete C file\n",
    "complete_c_content = create_complete_c_file(\n",
    "    original_c_file=\"linear_model.c\",\n",
    "    output_c_file=\"linear_model_with_main.c\",\n",
    "    X_df=X,\n",
    "    y_series=y,\n",
    "    n_samples=len(X)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17fdfaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcc linear_model_with_main.c -o test_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a521e8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model's r2_score (C): 0.15715596845899216\n",
      "Sample 1: True = 260972.16, Predicted = 213250.08\n",
      "Sample 2: True = 256534.25, Predicted = 199306.58\n",
      "Sample 3: True = 282674.29, Predicted = 264473.56\n",
      "Sample 4: True = 266555.38, Predicted = 226825.94\n",
      "Sample 5: True = 319158.42, Predicted = 283106.50\n"
     ]
    }
   ],
   "source": [
    "c_predictions = predict_and_eval(model=\"./test_model\", X=X, y=y, metric=r2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd2e9ff",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "723e19fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model's r2_score (Python): 0.15715598448124224\n",
      "Sample 1: True = 260972.16, Predicted = 213250.08\n",
      "Sample 2: True = 256534.25, Predicted = 199306.58\n",
      "Sample 3: True = 282674.29, Predicted = 264473.54\n",
      "Sample 4: True = 266555.38, Predicted = 226825.94\n",
      "Sample 5: True = 319158.42, Predicted = 283106.49\n"
     ]
    }
   ],
   "source": [
    "model = joblib.load(LIN_REG_PATH)\n",
    "py_predictions = predict_and_eval(model=model,X=X, y=y, metric=r2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a1a702",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70b241e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are C predictions and Python predictions the same? True\n"
     ]
    }
   ],
   "source": [
    "are_same = np.allclose(c_predictions, py_predictions, atol=1e-15)\n",
    "print(f\"Are C predictions and Python predictions the same? {are_same}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52229a55",
   "metadata": {},
   "source": [
    "**WE OBSERVE**:\n",
    "* **The same R^2 Score**\n",
    "* **For the same 5 first samples, the same predicted price (visual confirmation)**\n",
    "\n",
    "Similarity of predictions are confirmed by previous cell: Linear Regression was **successfully transpiled**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f11431",
   "metadata": {},
   "source": [
    "## Logistic Regression Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28e5a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(CANCER_DATA_PATH) \n",
    "X = data.drop(columns=[\"diagnosis\"])\n",
    "y = data[\"diagnosis\"].map({'M':1., 'B':0.})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc6f2dd",
   "metadata": {},
   "source": [
    "### C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc311e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from data/logistic_regression.joblib...\n",
      "\n",
      "C code generated and saved to: logistic_reg_model.c\n",
      "* Created complete C file: logistic_reg_model_with_main.c\n",
      "  - Original code from: logistic_reg_model.c\n",
      "  - Added main function with 569 test samples\n"
     ]
    }
   ],
   "source": [
    "transpiler = LinearModelTranspiler(LOG_REG_PATH, output_c_file=\"logistic_reg_model.c\")\n",
    "transpiler.transpile()\n",
    "\n",
    "# Create the complete C file\n",
    "complete_c_content = create_complete_c_file(\n",
    "    original_c_file=\"logistic_reg_model.c\",\n",
    "    output_c_file=\"logistic_reg_model_with_main.c\",\n",
    "    X_df=X,\n",
    "    y_series=y,\n",
    "    n_samples=len(X)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47e3aaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcc logistic_reg_model_with_main.c -o test_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "930d8687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model's accuracy_score (C): 0.9033391915641477\n",
      "Sample 1: True = 1.00, Predicted = 1.00\n",
      "Sample 2: True = 1.00, Predicted = 1.00\n",
      "Sample 3: True = 1.00, Predicted = 1.00\n",
      "Sample 4: True = 1.00, Predicted = 1.00\n",
      "Sample 5: True = 1.00, Predicted = 0.00\n"
     ]
    }
   ],
   "source": [
    "c_predictions = predict_and_eval(model=\"./test_model\", X=X,y=y, metric=accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73befcf0",
   "metadata": {},
   "source": [
    "### Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef838a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model's accuracy_score (Python): 0.9033391915641477\n",
      "Sample 1: True = 1.00, Predicted = 1.00\n",
      "Sample 2: True = 1.00, Predicted = 1.00\n",
      "Sample 3: True = 1.00, Predicted = 1.00\n",
      "Sample 4: True = 1.00, Predicted = 1.00\n",
      "Sample 5: True = 1.00, Predicted = 0.00\n"
     ]
    }
   ],
   "source": [
    "model = joblib.load(LOG_REG_PATH)\n",
    "py_predictions = predict_and_eval(model=model,X=X,y=y, metric=accuracy_score)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4940ab68",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fb25c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are C predictions and Python predictions the same? True\n"
     ]
    }
   ],
   "source": [
    "are_same = np.allclose(c_predictions, py_predictions[:len(c_predictions)], atol=1e-15)\n",
    "print(f\"Are C predictions and Python predictions the same? {are_same}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e63dcc",
   "metadata": {},
   "source": [
    "**AGAIN WE OBSERVE**:\n",
    "* **The same accuracy Score**\n",
    "* **For the same 5 first samples, the same predicted label (visual confirmation)**\n",
    "\n",
    "Similarity of predictions are confirmed by previous cell: Logistic Regression was **successfully transpiled**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567edac5",
   "metadata": {},
   "source": [
    "## Decision Trees Classifier Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f0ed0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CANCER_DATA_PATH)\n",
    "X = data.drop(columns=[\"diagnosis\"])\n",
    "y = data[\"diagnosis\"].map({'M':1., 'B':0.})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd4a7e6",
   "metadata": {},
   "source": [
    "### C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba7ee417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from data/tree_clf.joblib...\n",
      "\n",
      "C code generated and saved to: decision_tree_clf.c\n",
      "* Created complete C file: decision_tree_clf_with_main.c\n",
      "  - Original code from: decision_tree_clf.c\n",
      "  - Added main function with 569 test samples\n"
     ]
    }
   ],
   "source": [
    "transpiler = LinearModelTranspiler(model_path=DECISION_TREE_CLF_PATH, output_c_file=\"decision_tree_clf.c\")\n",
    "transpiler.transpile()\n",
    "\n",
    "complete_c_content = create_complete_c_file(\n",
    "    original_c_file=\"decision_tree_clf.c\",\n",
    "    output_c_file=\"decision_tree_clf_with_main.c\",\n",
    "    X_df=X,\n",
    "    y_series=y,\n",
    "    n_samples=len(X),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "78d9df65",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcc decision_tree_clf_with_main.c -o test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44791471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model's accuracy_score (C): 1.0\n",
      "Sample 1: True = 1.00, Predicted = 1.00\n",
      "Sample 2: True = 1.00, Predicted = 1.00\n",
      "Sample 3: True = 1.00, Predicted = 1.00\n",
      "Sample 4: True = 1.00, Predicted = 1.00\n",
      "Sample 5: True = 1.00, Predicted = 1.00\n"
     ]
    }
   ],
   "source": [
    "c_predictions = predict_and_eval(model=\"./test_model\", X=X, y=y, metric=accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb70124e",
   "metadata": {},
   "source": [
    "### Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3914b811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model's accuracy_score (Python): 1.0\n",
      "Sample 1: True = 1.00, Predicted = 1.00\n",
      "Sample 2: True = 1.00, Predicted = 1.00\n",
      "Sample 3: True = 1.00, Predicted = 1.00\n",
      "Sample 4: True = 1.00, Predicted = 1.00\n",
      "Sample 5: True = 1.00, Predicted = 1.00\n"
     ]
    }
   ],
   "source": [
    "model = joblib.load(DECISION_TREE_CLF_PATH)\n",
    "\n",
    "py_predictions = predict_and_eval(model=model, X=X, y=y, metric=accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e7295b",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b70a4628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are C predictions and Python predictions the same? True\n"
     ]
    }
   ],
   "source": [
    "are_same = np.allclose(c_predictions, py_predictions[:len(c_predictions)], atol=1e-15)\n",
    "print(f\"Are C predictions and Python predictions the same? {are_same}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2329457",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0664b6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(HOUSE_DATA_PATH)\n",
    "X = df.drop(columns=[\"price\", \"orientation\"])\n",
    "y = df[\"price\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579561c3",
   "metadata": {},
   "source": [
    "### C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b6311c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from data/tree_reg.joblib...\n",
      "\n",
      "C code generated and saved to: decision_tree_reg.c\n",
      "* Created complete C file: decision_tree_reg_with_main.c\n",
      "  - Original code from: decision_tree_reg.c\n",
      "  - Added main function with 40 test samples\n"
     ]
    }
   ],
   "source": [
    "transpiler = LinearModelTranspiler(DECISION_TREE_REG_PATH, output_c_file=\"decision_tree_reg.c\")\n",
    "transpiler.transpile()\n",
    "\n",
    "complete_c_content = create_complete_c_file(\n",
    "    original_c_file=\"decision_tree_reg.c\",\n",
    "    output_c_file=\"decision_tree_reg_with_main.c\",\n",
    "    X_df=X,\n",
    "    y_series=y,\n",
    "    n_samples=len(X),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19a33afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcc decision_tree_reg_with_main.c -o test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "761ef98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model's r2_score (C): 0.9999999999999961\n",
      "Sample 1: True = 260972.16, Predicted = 260972.17\n",
      "Sample 2: True = 256534.25, Predicted = 256534.25\n",
      "Sample 3: True = 282674.29, Predicted = 282674.28\n",
      "Sample 4: True = 266555.38, Predicted = 266555.38\n",
      "Sample 5: True = 319158.42, Predicted = 319158.41\n"
     ]
    }
   ],
   "source": [
    "c_predictions = predict_and_eval(model=\"./test_model\", X=X, y=y, metric=r2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe407ef",
   "metadata": {},
   "source": [
    "### Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71ef8e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model's r2_score (Python): 1.0\n",
      "Sample 1: True = 260972.16, Predicted = 260972.16\n",
      "Sample 2: True = 256534.25, Predicted = 256534.25\n",
      "Sample 3: True = 282674.29, Predicted = 282674.29\n",
      "Sample 4: True = 266555.38, Predicted = 266555.38\n",
      "Sample 5: True = 319158.42, Predicted = 319158.42\n"
     ]
    }
   ],
   "source": [
    "model = joblib.load(DECISION_TREE_REG_PATH)\n",
    "py_predictions = predict_and_eval(model=model, X=X, y=y, metric=r2_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af5a87c",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8829ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are C predictions and Python predictions the same? True\n"
     ]
    }
   ],
   "source": [
    "are_same = np.allclose(c_predictions, py_predictions[:len(c_predictions)], atol=1e-15)\n",
    "print(f\"Are C predictions and Python predictions the same? {are_same}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0030ef53",
   "metadata": {},
   "source": [
    "\n",
    "**In the same fashion, we can conclude from all those results that the transpiler works as intended for simple Decision Trees**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd0e6c3",
   "metadata": {},
   "source": [
    "## Conclusion:\n",
    "\n",
    "The transpiler works as intended"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
